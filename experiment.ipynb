{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "\tdata = pd.read_table(filename,names=['user_id','item_id','rates','time_stamp'])\n",
    "\ta = pd.pivot_table(\n",
    "\t\tdata,\n",
    "\t\tvalues='rates',\n",
    "\t\tindex=['user_id'],\n",
    "\t\tcolumns=['item_id'],\n",
    "\t\taggfunc=np.sum\n",
    "\t\t)\n",
    "\ta = np.array(a)\n",
    "\treturn a\n",
    "\n",
    "\t# return 1 \n",
    "\t# data_dict = defaultdict( lambda :defaultdict(lambda :0))\n",
    "\t# # user_id item_id rates\n",
    "\n",
    "\t# for line in data.values:\n",
    "\t# \tdata_dict[line[1]][line[0]] = line[2] \n",
    "\n",
    "\t# data_frame = pd.DataFrame(data_dict)\n",
    "\t# data_frame.fillna(0,inplace=True)\n",
    "\t# return data_frame\n",
    "\n",
    "def loss():\n",
    "\tpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train2(R,P,Q,K,steps=1000,alpha=0.002,beta=0.02):\n",
    "\tQ = Q.T\n",
    "\tfor step in range(steps):\n",
    "\t\tfor i in range(len(R)):\n",
    "\t\t\tfor j in range(len(R[i])):\n",
    "\t\t\t\tif R[i][j] > 0:\n",
    "\t\t\t\t\teij = R[i][j] - np.dot(P[i,:],Q[:,j])\n",
    "\t\t\t\t\tfor k in range(K):\n",
    "\t\t\t\t\t\tP[i][k] = P[i][k] + alpha * ( 2 * eij * Q[k][j] - beta * P[i][k])\n",
    "\t\t\t\t\t\tQ[k][j] = Q[k][j] + alpha * ( 2 * eij * P[i][k] - beta * Q[k][j])\n",
    "\t\teR = np.dot(P,Q)\n",
    "\t\te = 0\n",
    "\t\tfor i in range(len(R)):\n",
    "\t\t\tfor j in range(len(R[i])):\n",
    "\t\t\t\tif R[i][j] > 0:\n",
    "\t\t\t\t\te +=  pow(R[i][j] - np.dot(P[i,:],Q[:,j]),2)\n",
    "\t\t\t\t\tfor k in range(K):\n",
    "\t\t\t\t\t\te += (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2) )\n",
    "\t\tprint(e)\n",
    "\t\tif e < 0.001:\n",
    "\t\t\tbreak\n",
    "\treturn P,Q.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_list=[]\n",
    "step_list=[]\n",
    "def matrix_factorization(R,P,Q,K,test_data,steps=2, alpha=0.0002, beta=0.02,batch_size=800):\n",
    "\tQ = Q.T\n",
    "\tmin_size = min(test_data.shape[1] , R.shape[1])\n",
    "\tused_samples = np.zeros(min_size).reshape(-1)\n",
    "\tfor step in range(steps):\n",
    "\t\tif not(np.where(used_samples==0)[0].shape[0]):\n",
    "\t\t\tbreak\n",
    "\t\tidxs = np.random.choice(np.where(used_samples == 0)[0], batch_size)\n",
    "\t\tr = np.zeros(R.shape)\n",
    "\t\tr[:, idxs] = R[:, idxs]\n",
    "\t\tt = np.zeros(test_data.shape)\n",
    "\t\tt[:, idxs] = test_data[:, idxs]\n",
    "\n",
    "\t\tfor i in range(len(r)):\n",
    "\t\t\tfor j in range(len(r[i])):\n",
    "\t\t\t\tif r[i][j] > 0:\n",
    "\t\t\t\t\teij = r[i][j] - np.dot(P[i, :], Q[:, j])  # 计算loss\n",
    "\t\t\t\t\tfor k in range(K):  # 更新参数\n",
    "\t\t\t\t\t\tP[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "\t\t\t\t\t\tQ[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "\t\te = 0\n",
    "\t\tfor i in range(len(t)):\n",
    "\t\t\tfor j in range(len(t[i])):\n",
    "\t\t\t\tif t[i][j] > 0:\n",
    "\t\t\t\t\te = e + pow(t[i][j] - np.dot(P[i, :], Q[:, j]), 2)\n",
    "\t\t\t\t\tfor k in range(K):\n",
    "\t\t\t\t\t\te = e + (beta / 2) * (pow(P[i][k], 2) + pow(Q[k][j], 2))\n",
    "\t\tloss = e / (len(test_data) * len(test_data[0]))\n",
    "\t\tloss_list.append(loss)\n",
    "\t\tstep_list.append(step)\n",
    "\t\tprint(\"step:\", step, \"loss:\", loss)\n",
    "\t\tif e < 0.001:\n",
    "\t\t\tbreak\n",
    "\treturn P, Q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 loss: 0.0792641364277\n",
      "step: 1 loss: 0.0745289116665\n",
      "step: 2 loss: 0.0693794655884\n",
      "step: 3 loss: 0.0623980088144\n",
      "step: 4 loss: 0.0556166025751\n",
      "step: 5 loss: 0.0515015981434\n",
      "step: 6 loss: 0.0467434346417\n",
      "step: 7 loss: 0.0454156053893\n",
      "step: 8 loss: 0.0442685943551\n",
      "step: 9 loss: 0.04113381958\n",
      "step: 10 loss: 0.037679367567\n",
      "step: 11 loss: 0.036466292101\n",
      "step: 12 loss: 0.0370049635677\n",
      "step: 13 loss: 0.0334479621354\n",
      "step: 14 loss: 0.0314886054373\n",
      "step: 15 loss: 0.0336353620454\n",
      "step: 16 loss: 0.032062917681\n",
      "step: 17 loss: 0.0277601115574\n",
      "step: 18 loss: 0.02869281061\n",
      "step: 19 loss: 0.0284308801062\n",
      "step: 20 loss: 0.0274357387387\n",
      "step: 21 loss: 0.0275635866527\n",
      "step: 22 loss: 0.0250051413256\n",
      "step: 23 loss: 0.0269740943975\n",
      "step: 24 loss: 0.0261475206593\n",
      "step: 25 loss: 0.0252941747206\n",
      "step: 26 loss: 0.0262380247225\n",
      "step: 27 loss: 0.022155600522\n",
      "step: 28 loss: 0.0251638586619\n",
      "step: 29 loss: 0.0240829628185\n",
      "step: 30 loss: 0.0233814365455\n",
      "step: 31 loss: 0.0217282520191\n",
      "step: 32 loss: 0.0235634348985\n",
      "step: 33 loss: 0.0233232564468\n",
      "step: 34 loss: 0.0217336344594\n",
      "step: 35 loss: 0.0232907236275\n",
      "step: 36 loss: 0.022048363045\n",
      "step: 37 loss: 0.022675901468\n",
      "step: 38 loss: 0.0223048548058\n",
      "step: 39 loss: 0.0232236988625\n",
      "step: 40 loss: 0.0216087847203\n",
      "step: 41 loss: 0.0211384997943\n",
      "step: 42 loss: 0.0215240813731\n",
      "step: 43 loss: 0.0206811736109\n",
      "step: 44 loss: 0.0207771814272\n",
      "step: 45 loss: 0.020761129378\n",
      "step: 46 loss: 0.0215345661232\n",
      "step: 47 loss: 0.0200922833211\n",
      "step: 48 loss: 0.0210194801194\n",
      "step: 49 loss: 0.021075557073\n",
      "step: 50 loss: 0.0196947159997\n",
      "step: 51 loss: 0.020647851274\n",
      "step: 52 loss: 0.0208794086354\n",
      "step: 53 loss: 0.0192943194521\n",
      "step: 54 loss: 0.0204665483205\n",
      "step: 55 loss: 0.0191724219103\n",
      "step: 56 loss: 0.0203073506132\n",
      "step: 57 loss: 0.0189643400541\n",
      "step: 58 loss: 0.0199395460748\n",
      "step: 59 loss: 0.0193069035303\n",
      "step: 60 loss: 0.0187798086657\n",
      "step: 61 loss: 0.0213956154487\n",
      "step: 62 loss: 0.0185599389297\n",
      "step: 63 loss: 0.02022254245\n",
      "step: 64 loss: 0.0184205651393\n",
      "step: 65 loss: 0.0193913245786\n",
      "step: 66 loss: 0.0205968382418\n",
      "step: 67 loss: 0.0184601399488\n",
      "step: 68 loss: 0.0207395625234\n",
      "step: 69 loss: 0.0200118846201\n",
      "step: 70 loss: 0.0182260046843\n",
      "step: 71 loss: 0.0191045760054\n",
      "step: 72 loss: 0.0194519085177\n",
      "step: 73 loss: 0.0181990669305\n",
      "step: 74 loss: 0.0196929069081\n",
      "step: 75 loss: 0.0185416952427\n",
      "step: 76 loss: 0.0176764687622\n",
      "step: 77 loss: 0.0180727096379\n",
      "step: 78 loss: 0.0179804462237\n",
      "step: 79 loss: 0.018978803161\n",
      "step: 80 loss: 0.0180963154929\n",
      "step: 81 loss: 0.0191794681586\n",
      "step: 82 loss: 0.0174478397348\n",
      "step: 83 loss: 0.0196702631515\n",
      "step: 84 loss: 0.018595260158\n",
      "step: 85 loss: 0.0189778552141\n",
      "step: 86 loss: 0.0177828510938\n",
      "step: 87 loss: 0.018814568152\n",
      "step: 88 loss: 0.0197975800573\n",
      "step: 89 loss: 0.0181831197442\n",
      "step: 90 loss: 0.0189654530473\n",
      "step: 91 loss: 0.0180867865142\n",
      "step: 92 loss: 0.0182526135922\n",
      "step: 93 loss: 0.0194100264686\n",
      "step: 94 loss: 0.0188343797889\n",
      "step: 95 loss: 0.0183211485758\n",
      "step: 96 loss: 0.0186986363738\n",
      "step: 97 loss: 0.0182387865414\n",
      "step: 98 loss: 0.0185821448642\n",
      "step: 99 loss: 0.0179405335423\n",
      "step: 100 loss: 0.0188873677235\n",
      "step: 101 loss: 0.0179208640983\n",
      "step: 102 loss: 0.019222234954\n",
      "step: 103 loss: 0.0188316458021\n",
      "step: 104 loss: 0.0174784924881\n",
      "step: 105 loss: 0.0182310153412\n",
      "step: 106 loss: 0.0188791567729\n",
      "step: 107 loss: 0.016013542918\n",
      "step: 108 loss: 0.0179708665258\n",
      "step: 109 loss: 0.0171285324288\n",
      "step: 110 loss: 0.0179520493029\n",
      "step: 111 loss: 0.0184634479483\n",
      "step: 112 loss: 0.0175809846894\n",
      "step: 113 loss: 0.0173872562521\n",
      "step: 114 loss: 0.0181065373416\n",
      "step: 115 loss: 0.0181127669763\n",
      "step: 116 loss: 0.0181539836125\n",
      "step: 117 loss: 0.0170614172705\n",
      "step: 118 loss: 0.0174647877382\n",
      "step: 119 loss: 0.0189573707366\n",
      "step: 120 loss: 0.0194318918344\n",
      "step: 121 loss: 0.0163027851302\n",
      "step: 122 loss: 0.0178683360523\n",
      "step: 123 loss: 0.0168946130837\n",
      "step: 124 loss: 0.0183986609049\n",
      "step: 125 loss: 0.0181028241456\n",
      "step: 126 loss: 0.0175345633206\n",
      "step: 127 loss: 0.0188806937205\n",
      "step: 128 loss: 0.0180063634272\n",
      "step: 129 loss: 0.0166400344622\n",
      "step: 130 loss: 0.017252773975\n",
      "step: 131 loss: 0.0183812020426\n",
      "step: 132 loss: 0.0176442784439\n",
      "step: 133 loss: 0.0180327981455\n",
      "step: 134 loss: 0.0165306058389\n",
      "step: 135 loss: 0.018169385372\n",
      "step: 136 loss: 0.0175923566676\n",
      "step: 137 loss: 0.0169012302185\n",
      "step: 138 loss: 0.0168218706959\n",
      "step: 139 loss: 0.0175872446707\n",
      "step: 140 loss: 0.0166600295853\n",
      "step: 141 loss: 0.0175851277726\n",
      "step: 142 loss: 0.0178070770369\n",
      "step: 143 loss: 0.017255542929\n",
      "step: 144 loss: 0.0178669342354\n",
      "step: 145 loss: 0.017761024211\n",
      "step: 146 loss: 0.0168120128036\n",
      "step: 147 loss: 0.016976098774\n",
      "step: 148 loss: 0.0176012631125\n",
      "step: 149 loss: 0.0188653794142\n",
      "step: 150 loss: 0.0172775770064\n",
      "step: 151 loss: 0.0178705724626\n",
      "step: 152 loss: 0.0173516480596\n",
      "step: 153 loss: 0.0165430907351\n",
      "step: 154 loss: 0.0167809265066\n",
      "step: 155 loss: 0.0170393271225\n",
      "step: 156 loss: 0.0177506425379\n",
      "step: 157 loss: 0.0176041088055\n",
      "step: 158 loss: 0.0194066684614\n",
      "step: 159 loss: 0.0172544257253\n",
      "step: 160 loss: 0.0169537717277\n",
      "step: 161 loss: 0.0177493736713\n",
      "step: 162 loss: 0.0188617518346\n",
      "step: 163 loss: 0.0174967348292\n",
      "step: 164 loss: 0.0160309081875\n",
      "step: 165 loss: 0.018110354905\n",
      "step: 166 loss: 0.0180044784674\n",
      "step: 167 loss: 0.0179346448038\n",
      "step: 168 loss: 0.0172653573658\n",
      "step: 169 loss: 0.0181676561165\n",
      "step: 170 loss: 0.0180759415802\n",
      "step: 171 loss: 0.0168495386035\n",
      "step: 172 loss: 0.017572620364\n",
      "step: 173 loss: 0.0161628277691\n",
      "step: 174 loss: 0.0181014427373\n",
      "step: 175 loss: 0.0174965848762\n",
      "step: 176 loss: 0.016498034494\n",
      "step: 177 loss: 0.0178384906291\n",
      "step: 178 loss: 0.0180087232985\n",
      "step: 179 loss: 0.0176877746831\n",
      "step: 180 loss: 0.0169345115162\n",
      "step: 181 loss: 0.0171541698793\n",
      "step: 182 loss: 0.0161969119157\n",
      "step: 183 loss: 0.0167356983456\n",
      "step: 184 loss: 0.0168710767857\n",
      "step: 185 loss: 0.0181461467576\n",
      "step: 186 loss: 0.0185220186464\n",
      "step: 187 loss: 0.0169278017952\n",
      "step: 188 loss: 0.0166520689653\n",
      "step: 189 loss: 0.0167563976709\n",
      "step: 190 loss: 0.0167679797694\n",
      "step: 191 loss: 0.0181723824343\n",
      "step: 192 loss: 0.0166446846612\n",
      "step: 193 loss: 0.0166831202067\n",
      "step: 194 loss: 0.0166408469072\n",
      "step: 195 loss: 0.0166731100785\n",
      "step: 196 loss: 0.0172616212965\n",
      "step: 197 loss: 0.0169927407726\n",
      "step: 198 loss: 0.0166208335807\n",
      "step: 199 loss: 0.0187784020134\n",
      "step: 200 loss: 0.0165372617359\n",
      "step: 201 loss: 0.0181328259703\n",
      "step: 202 loss: 0.0177350937817\n",
      "step: 203 loss: 0.0161369438182\n",
      "step: 204 loss: 0.0162454036982\n",
      "step: 205 loss: 0.0168746649597\n",
      "step: 206 loss: 0.0174260783874\n",
      "step: 207 loss: 0.0160160518568\n",
      "step: 208 loss: 0.0185979894604\n",
      "step: 209 loss: 0.0176791923634\n",
      "step: 210 loss: 0.0171319535754\n",
      "step: 211 loss: 0.0182116758583\n",
      "step: 212 loss: 0.0159675304736\n",
      "step: 213 loss: 0.0177419853002\n",
      "step: 214 loss: 0.0167380913863\n",
      "step: 215 loss: 0.0166571314819\n",
      "step: 216 loss: 0.0162470502306\n",
      "step: 217 loss: 0.0166813204926\n",
      "step: 218 loss: 0.016990057234\n",
      "step: 219 loss: 0.0174954619891\n",
      "step: 220 loss: 0.0171118342146\n",
      "step: 221 loss: 0.0169204926896\n",
      "step: 222 loss: 0.0174421253022\n",
      "step: 223 loss: 0.0171606599804\n",
      "step: 224 loss: 0.0183031239795\n",
      "step: 225 loss: 0.017464173289\n",
      "step: 226 loss: 0.0165799655879\n",
      "step: 227 loss: 0.0178393573869\n",
      "step: 228 loss: 0.0161550097055\n",
      "step: 229 loss: 0.0166469724626\n",
      "step: 230 loss: 0.0168223862611\n",
      "step: 231 loss: 0.0178310979341\n",
      "step: 232 loss: 0.0173300164453\n",
      "step: 233 loss: 0.0175556813239\n",
      "step: 234 loss: 0.0180322854784\n",
      "step: 235 loss: 0.0162662377651\n",
      "step: 236 loss: 0.0173976293966\n",
      "step: 237 loss: 0.0163867285861\n",
      "step: 238 loss: 0.0169895703259\n",
      "step: 239 loss: 0.016423880479\n",
      "step: 240 loss: 0.0173292992907\n",
      "step: 241 loss: 0.0160940588958\n",
      "step: 242 loss: 0.0161771588718\n",
      "step: 243 loss: 0.0170535605206\n",
      "step: 244 loss: 0.0165750120123\n",
      "step: 245 loss: 0.0173670656512\n",
      "step: 246 loss: 0.0164111239517\n",
      "step: 247 loss: 0.0170392820397\n",
      "step: 248 loss: 0.0168621194938\n",
      "step: 249 loss: 0.0168466628989\n",
      "step: 250 loss: 0.0161551484736\n",
      "step: 251 loss: 0.0177376429762\n",
      "step: 252 loss: 0.0175402742164\n",
      "step: 253 loss: 0.0170075578016\n",
      "step: 254 loss: 0.0175265293207\n",
      "step: 255 loss: 0.0162928206044\n",
      "step: 256 loss: 0.0174627900315\n",
      "step: 257 loss: 0.0163746380825\n",
      "step: 258 loss: 0.0171131666714\n",
      "step: 259 loss: 0.0170476085255\n",
      "step: 260 loss: 0.0163513058108\n",
      "step: 261 loss: 0.0171214778684\n",
      "step: 262 loss: 0.0160646901878\n",
      "step: 263 loss: 0.0163846806835\n",
      "step: 264 loss: 0.0164677321103\n",
      "step: 265 loss: 0.0175740375772\n",
      "step: 266 loss: 0.0169935872184\n",
      "step: 267 loss: 0.0162888674127\n",
      "step: 268 loss: 0.0165570057147\n",
      "step: 269 loss: 0.0165431322516\n",
      "step: 270 loss: 0.0173768947438\n",
      "step: 271 loss: 0.0174067259237\n",
      "step: 272 loss: 0.0168012348647\n",
      "step: 273 loss: 0.0165950791735\n",
      "step: 274 loss: 0.0170778031472\n",
      "step: 275 loss: 0.0161292293404\n",
      "step: 276 loss: 0.0192670516751\n",
      "step: 277 loss: 0.0152578371047\n",
      "step: 278 loss: 0.0178190612331\n",
      "step: 279 loss: 0.0172820494691\n",
      "step: 280 loss: 0.0167583464406\n",
      "step: 281 loss: 0.0175855676743\n",
      "step: 282 loss: 0.0170444944967\n",
      "step: 283 loss: 0.0173678051031\n",
      "step: 284 loss: 0.0171897184895\n",
      "step: 285 loss: 0.0176393810888\n",
      "step: 286 loss: 0.0167284754967\n",
      "step: 287 loss: 0.0180496118498\n",
      "step: 288 loss: 0.0151932454458\n",
      "step: 289 loss: 0.0184349927398\n",
      "step: 290 loss: 0.0176370698116\n",
      "step: 291 loss: 0.0165169342983\n",
      "step: 292 loss: 0.018109974024\n",
      "step: 293 loss: 0.0164655267417\n",
      "step: 294 loss: 0.0178845995781\n",
      "step: 295 loss: 0.0180754006651\n",
      "step: 296 loss: 0.0176917776688\n",
      "step: 297 loss: 0.0182140104593\n",
      "step: 298 loss: 0.0157397302106\n",
      "step: 299 loss: 0.0163644740121\n",
      "step: 300 loss: 0.0170031851734\n",
      "step: 301 loss: 0.0167846657981\n",
      "step: 302 loss: 0.0175321728852\n",
      "step: 303 loss: 0.0164318247772\n",
      "step: 304 loss: 0.0173922746442\n",
      "step: 305 loss: 0.0156785119615\n",
      "step: 306 loss: 0.0167881719182\n",
      "step: 307 loss: 0.0177444279291\n",
      "step: 308 loss: 0.0163834085149\n",
      "step: 309 loss: 0.0167431196038\n",
      "step: 310 loss: 0.0168003429998\n",
      "step: 311 loss: 0.0168504788904\n",
      "step: 312 loss: 0.0183771128331\n",
      "step: 313 loss: 0.0164426015718\n",
      "step: 314 loss: 0.0161930296996\n",
      "step: 315 loss: 0.0166500535067\n",
      "step: 316 loss: 0.0176560332333\n",
      "step: 317 loss: 0.0168421904416\n",
      "step: 318 loss: 0.0164934011665\n",
      "step: 319 loss: 0.0184787228762\n",
      "step: 320 loss: 0.0175509869302\n",
      "step: 321 loss: 0.0165892023351\n",
      "step: 322 loss: 0.0171180597944\n",
      "step: 323 loss: 0.0171484760526\n",
      "step: 324 loss: 0.016433528986\n",
      "step: 325 loss: 0.0178597570905\n",
      "step: 326 loss: 0.0170866703382\n",
      "step: 327 loss: 0.0174708452329\n",
      "step: 328 loss: 0.0171951690896\n",
      "step: 329 loss: 0.0170023136671\n",
      "step: 330 loss: 0.0161441385017\n",
      "step: 331 loss: 0.0171952236144\n",
      "step: 332 loss: 0.017525171472\n",
      "step: 333 loss: 0.0161648554466\n",
      "step: 334 loss: 0.0161558293461\n",
      "step: 335 loss: 0.015546096899\n",
      "step: 336 loss: 0.0166714114095\n",
      "step: 337 loss: 0.0172187414713\n",
      "step: 338 loss: 0.0180924462917\n",
      "step: 339 loss: 0.0159869730485\n",
      "step: 340 loss: 0.0173180429779\n",
      "step: 341 loss: 0.0172831359364\n",
      "step: 342 loss: 0.01713758063\n",
      "step: 343 loss: 0.0187184399344\n",
      "step: 344 loss: 0.0163345258484\n",
      "step: 345 loss: 0.0156348181435\n",
      "step: 346 loss: 0.0156448446296\n",
      "step: 347 loss: 0.0174637814411\n",
      "step: 348 loss: 0.0172064447805\n",
      "step: 349 loss: 0.0171272449285\n",
      "step: 350 loss: 0.0154029303785\n",
      "step: 351 loss: 0.0157872988494\n",
      "step: 352 loss: 0.0170009549265\n",
      "step: 353 loss: 0.0174063510533\n",
      "step: 354 loss: 0.0159108550303\n",
      "step: 355 loss: 0.016790844452\n",
      "step: 356 loss: 0.0172712834823\n",
      "step: 357 loss: 0.0164296626855\n",
      "step: 358 loss: 0.0165713768206\n",
      "step: 359 loss: 0.0163927692629\n",
      "step: 360 loss: 0.017185390698\n",
      "step: 361 loss: 0.0155402733381\n",
      "step: 362 loss: 0.0165666547529\n",
      "step: 363 loss: 0.0167876698225\n",
      "step: 364 loss: 0.0168807819036\n",
      "step: 365 loss: 0.0172212211421\n",
      "step: 366 loss: 0.016759270746\n",
      "step: 367 loss: 0.0168717856198\n",
      "step: 368 loss: 0.0165472265587\n",
      "step: 369 loss: 0.0160733591678\n",
      "step: 370 loss: 0.0165025953354\n",
      "step: 371 loss: 0.0162191494957\n",
      "step: 372 loss: 0.0165079378858\n",
      "step: 373 loss: 0.0162365087977\n",
      "step: 374 loss: 0.0164143382004\n",
      "step: 375 loss: 0.0169469043511\n",
      "step: 376 loss: 0.0177219595567\n",
      "step: 377 loss: 0.0167316252014\n",
      "step: 378 loss: 0.0173656558611\n",
      "step: 379 loss: 0.0158024469233\n",
      "step: 380 loss: 0.0164159451202\n",
      "step: 381 loss: 0.0164616610607\n",
      "step: 382 loss: 0.0175969504475\n",
      "step: 383 loss: 0.0171387982685\n",
      "step: 384 loss: 0.0172973129176\n",
      "step: 385 loss: 0.0161288770343\n",
      "step: 386 loss: 0.0162379746158\n",
      "step: 387 loss: 0.0175837582711\n",
      "step: 388 loss: 0.0165578378399\n",
      "step: 389 loss: 0.0151972002102\n",
      "step: 390 loss: 0.0185398262093\n",
      "step: 391 loss: 0.0174495987311\n",
      "step: 392 loss: 0.0162481841803\n",
      "step: 393 loss: 0.0176026430419\n",
      "step: 394 loss: 0.0171419169389\n",
      "step: 395 loss: 0.0176773069797\n",
      "step: 396 loss: 0.0176719589998\n",
      "step: 397 loss: 0.0163991463784\n",
      "step: 398 loss: 0.0175553158963\n",
      "step: 399 loss: 0.0174816928804\n",
      "step: 400 loss: 0.0162991916108\n",
      "step: 401 loss: 0.0162337487967\n",
      "step: 402 loss: 0.0169088475379\n",
      "step: 403 loss: 0.0178789737577\n",
      "step: 404 loss: 0.0169156087142\n",
      "step: 405 loss: 0.0177913188673\n",
      "step: 406 loss: 0.0159010149215\n",
      "step: 407 loss: 0.0168525837666\n",
      "step: 408 loss: 0.0162719083713\n",
      "step: 409 loss: 0.0182248433974\n",
      "step: 410 loss: 0.0171706110197\n",
      "step: 411 loss: 0.0176005967211\n",
      "step: 412 loss: 0.0175657100465\n",
      "step: 413 loss: 0.016383332222\n",
      "step: 414 loss: 0.0178326195339\n",
      "step: 415 loss: 0.0183089680438\n",
      "step: 416 loss: 0.0160567933904\n",
      "step: 417 loss: 0.0164192309627\n",
      "step: 418 loss: 0.0157037109805\n",
      "step: 419 loss: 0.016747213677\n",
      "step: 420 loss: 0.0159268902649\n",
      "step: 421 loss: 0.0180747972451\n",
      "step: 422 loss: 0.0183404634117\n",
      "step: 423 loss: 0.0172264298273\n",
      "step: 424 loss: 0.0166244238134\n",
      "step: 425 loss: 0.0171130015111\n",
      "step: 426 loss: 0.0176030802756\n",
      "step: 427 loss: 0.0181654926754\n",
      "step: 428 loss: 0.016074020819\n",
      "step: 429 loss: 0.0171006045948\n",
      "step: 430 loss: 0.0176799041382\n",
      "step: 431 loss: 0.0173662437394\n",
      "step: 432 loss: 0.0160899707001\n",
      "step: 433 loss: 0.0176635314609\n",
      "step: 434 loss: 0.0171930402116\n",
      "step: 435 loss: 0.0169729933543\n",
      "step: 436 loss: 0.0160944855725\n",
      "step: 437 loss: 0.0172607941787\n",
      "step: 438 loss: 0.0172652824155\n",
      "step: 439 loss: 0.0179293068064\n",
      "step: 440 loss: 0.0160445985976\n",
      "step: 441 loss: 0.017261971299\n",
      "step: 442 loss: 0.017596286822\n",
      "step: 443 loss: 0.0168976166307\n",
      "step: 444 loss: 0.017788948767\n",
      "step: 445 loss: 0.0173935229667\n",
      "step: 446 loss: 0.016994500853\n",
      "step: 447 loss: 0.0166522540293\n",
      "step: 448 loss: 0.01744156506\n",
      "step: 449 loss: 0.0177543196445\n",
      "step: 450 loss: 0.01633873891\n",
      "step: 451 loss: 0.0170606293714\n",
      "step: 452 loss: 0.0165209927231\n",
      "step: 453 loss: 0.0170463161366\n",
      "step: 454 loss: 0.0169685833059\n",
      "step: 455 loss: 0.016275443313\n",
      "step: 456 loss: 0.0190389825745\n",
      "step: 457 loss: 0.017127356645\n",
      "step: 458 loss: 0.0166844339893\n",
      "step: 459 loss: 0.0171997337115\n",
      "step: 460 loss: 0.0163858059777\n",
      "step: 461 loss: 0.0163865256185\n",
      "step: 462 loss: 0.0177857828362\n",
      "step: 463 loss: 0.0169348728975\n",
      "step: 464 loss: 0.0185311604501\n",
      "step: 465 loss: 0.0180887341584\n",
      "step: 466 loss: 0.016926826116\n",
      "step: 467 loss: 0.0160089161244\n",
      "step: 468 loss: 0.0168661113023\n",
      "step: 469 loss: 0.0166191291536\n",
      "step: 470 loss: 0.0167004125285\n",
      "step: 471 loss: 0.0173152224619\n",
      "step: 472 loss: 0.0175835725962\n",
      "step: 473 loss: 0.0166824133188\n",
      "step: 474 loss: 0.0163499603101\n",
      "step: 475 loss: 0.0164500938689\n",
      "step: 476 loss: 0.0171855937658\n",
      "step: 477 loss: 0.0162926366658\n",
      "step: 478 loss: 0.0168559902638\n",
      "step: 479 loss: 0.0162930233574\n",
      "step: 480 loss: 0.0168161830934\n",
      "step: 481 loss: 0.0164592395891\n",
      "step: 482 loss: 0.0169770621046\n",
      "step: 483 loss: 0.0179368307233\n",
      "step: 484 loss: 0.01683567783\n",
      "step: 485 loss: 0.0164568511644\n",
      "step: 486 loss: 0.0172488622314\n",
      "step: 487 loss: 0.0174660502758\n",
      "step: 488 loss: 0.0167818573539\n",
      "step: 489 loss: 0.0168080070434\n",
      "step: 490 loss: 0.0171120270627\n",
      "step: 491 loss: 0.016382729117\n",
      "step: 492 loss: 0.0172040095968\n",
      "step: 493 loss: 0.0179930088013\n",
      "step: 494 loss: 0.0165436501904\n",
      "step: 495 loss: 0.0154095658886\n",
      "step: 496 loss: 0.0173116964226\n",
      "step: 497 loss: 0.017486213045\n",
      "step: 498 loss: 0.0164960522847\n",
      "step: 499 loss: 0.0166318166448\n",
      "step: 500 loss: 0.0171533403322\n",
      "step: 501 loss: 0.016212029476\n",
      "step: 502 loss: 0.0189416804395\n",
      "step: 503 loss: 0.0175744682067\n",
      "step: 504 loss: 0.0159750916013\n",
      "step: 505 loss: 0.0164310004508\n",
      "step: 506 loss: 0.0175410693474\n",
      "step: 507 loss: 0.0173704011505\n",
      "step: 508 loss: 0.0168672171159\n",
      "step: 509 loss: 0.0164858755658\n",
      "step: 510 loss: 0.0162450149549\n",
      "step: 511 loss: 0.0172748555721\n",
      "step: 512 loss: 0.0174719491742\n",
      "step: 513 loss: 0.0172859884319\n",
      "step: 514 loss: 0.0171176113325\n",
      "step: 515 loss: 0.0177704379169\n",
      "step: 516 loss: 0.0175551821454\n",
      "step: 517 loss: 0.0164803091101\n",
      "step: 518 loss: 0.0165772036247\n",
      "step: 519 loss: 0.0179623183461\n",
      "step: 520 loss: 0.0173041654748\n",
      "step: 521 loss: 0.0190986921394\n",
      "step: 522 loss: 0.0157789218579\n",
      "step: 523 loss: 0.0172670825776\n",
      "step: 524 loss: 0.0175050180585\n",
      "step: 525 loss: 0.017280584094\n",
      "step: 526 loss: 0.0174232761258\n",
      "step: 527 loss: 0.0171053776423\n",
      "step: 528 loss: 0.0165924768968\n",
      "step: 529 loss: 0.0169166491615\n",
      "step: 530 loss: 0.017711688521\n",
      "step: 531 loss: 0.017301808403\n",
      "step: 532 loss: 0.017011073297\n",
      "step: 533 loss: 0.0182303501994\n",
      "step: 534 loss: 0.0159960797055\n",
      "step: 535 loss: 0.0179695682492\n",
      "step: 536 loss: 0.0173000383116\n",
      "step: 537 loss: 0.0159831907393\n",
      "step: 538 loss: 0.0167487561031\n",
      "step: 539 loss: 0.0160877999793\n",
      "step: 540 loss: 0.0165377060259\n",
      "step: 541 loss: 0.0170131109577\n",
      "step: 542 loss: 0.0158653544508\n",
      "step: 543 loss: 0.0173133366514\n",
      "step: 544 loss: 0.0177693738394\n",
      "step: 545 loss: 0.0164692708654\n",
      "step: 546 loss: 0.0172204401351\n",
      "step: 547 loss: 0.0167806250755\n",
      "step: 548 loss: 0.0181745680898\n",
      "step: 549 loss: 0.0149924804059\n",
      "step: 550 loss: 0.0174993264409\n",
      "step: 551 loss: 0.0161203465003\n",
      "step: 552 loss: 0.0169947634918\n",
      "step: 553 loss: 0.0172659783172\n",
      "step: 554 loss: 0.0164392849288\n",
      "step: 555 loss: 0.0171586164705\n",
      "step: 556 loss: 0.0174648712521\n",
      "step: 557 loss: 0.0172565275936\n",
      "step: 558 loss: 0.0164686589765\n",
      "step: 559 loss: 0.0173556208463\n",
      "step: 560 loss: 0.0184113175911\n",
      "step: 561 loss: 0.016623014623\n",
      "step: 562 loss: 0.0161561414923\n",
      "step: 563 loss: 0.0182717509281\n",
      "step: 564 loss: 0.0179866865555\n",
      "step: 565 loss: 0.0171317749769\n",
      "step: 566 loss: 0.0180512529189\n",
      "step: 567 loss: 0.0166414141685\n",
      "step: 568 loss: 0.016288451221\n",
      "step: 569 loss: 0.0171462654708\n",
      "step: 570 loss: 0.0177459078375\n",
      "step: 571 loss: 0.0160825691572\n",
      "step: 572 loss: 0.018090884114\n",
      "step: 573 loss: 0.016291395164\n",
      "step: 574 loss: 0.0153109101874\n",
      "step: 575 loss: 0.0167770399521\n",
      "step: 576 loss: 0.0159417475393\n",
      "step: 577 loss: 0.0175591529862\n",
      "step: 578 loss: 0.0166155363231\n",
      "step: 579 loss: 0.0166431014475\n",
      "step: 580 loss: 0.016183378307\n",
      "step: 581 loss: 0.0163773960958\n",
      "step: 582 loss: 0.016079669137\n",
      "step: 583 loss: 0.0166731478892\n",
      "step: 584 loss: 0.016682699753\n",
      "step: 585 loss: 0.0181464456062\n",
      "step: 586 loss: 0.0169877991786\n",
      "step: 587 loss: 0.0165639429903\n",
      "step: 588 loss: 0.0158637023132\n",
      "step: 589 loss: 0.015702218064\n",
      "step: 590 loss: 0.0166434111579\n",
      "step: 591 loss: 0.0178630941033\n",
      "step: 592 loss: 0.0162976969359\n",
      "step: 593 loss: 0.0163412814413\n",
      "step: 594 loss: 0.0166821577119\n",
      "step: 595 loss: 0.0164040720992\n",
      "step: 596 loss: 0.0174259861502\n",
      "step: 597 loss: 0.0170332532992\n",
      "step: 598 loss: 0.0178352781053\n",
      "step: 599 loss: 0.0162395007861\n",
      "step: 600 loss: 0.0182812570085\n",
      "step: 601 loss: 0.0181773253538\n",
      "step: 602 loss: 0.0176370386165\n",
      "step: 603 loss: 0.0171049687014\n",
      "step: 604 loss: 0.01804739899\n",
      "step: 605 loss: 0.0171834834926\n",
      "step: 606 loss: 0.0170118392848\n",
      "step: 607 loss: 0.0169047222396\n",
      "step: 608 loss: 0.0170638394133\n",
      "step: 609 loss: 0.0159950463042\n",
      "step: 610 loss: 0.0162053565227\n",
      "step: 611 loss: 0.0181679831093\n",
      "step: 612 loss: 0.0168968790972\n",
      "step: 613 loss: 0.0178021330203\n",
      "step: 614 loss: 0.0171645259962\n",
      "step: 615 loss: 0.0164436645763\n",
      "step: 616 loss: 0.0172256156268\n",
      "step: 617 loss: 0.0174908264116\n",
      "step: 618 loss: 0.0175716468859\n",
      "step: 619 loss: 0.016997874332\n",
      "step: 620 loss: 0.0165179456241\n",
      "step: 621 loss: 0.0178457314211\n",
      "step: 622 loss: 0.0173200739213\n",
      "step: 623 loss: 0.0176211872579\n",
      "step: 624 loss: 0.0166048413539\n",
      "step: 625 loss: 0.0170639685668\n",
      "step: 626 loss: 0.0179813639217\n",
      "step: 627 loss: 0.0173329103656\n",
      "step: 628 loss: 0.0185083122477\n",
      "step: 629 loss: 0.0154640632952\n",
      "step: 630 loss: 0.0171440780966\n",
      "step: 631 loss: 0.0167472858604\n",
      "step: 632 loss: 0.0170549574588\n",
      "step: 633 loss: 0.015735170091\n",
      "step: 634 loss: 0.0166005369598\n",
      "step: 635 loss: 0.0176617042751\n",
      "step: 636 loss: 0.0158870575791\n",
      "step: 637 loss: 0.0171354086803\n",
      "step: 638 loss: 0.0166843075892\n",
      "step: 639 loss: 0.0177351329982\n",
      "step: 640 loss: 0.0177063751723\n",
      "step: 641 loss: 0.0177029036424\n",
      "step: 642 loss: 0.0175333432455\n",
      "step: 643 loss: 0.0169070103732\n",
      "step: 644 loss: 0.0155565222671\n",
      "step: 645 loss: 0.0162376906248\n",
      "step: 646 loss: 0.0171337265544\n",
      "step: 647 loss: 0.0172259745923\n",
      "step: 648 loss: 0.0168363209926\n",
      "step: 649 loss: 0.0175874039001\n",
      "step: 650 loss: 0.0167020100369\n",
      "step: 651 loss: 0.0167345382988\n",
      "step: 652 loss: 0.016639054281\n",
      "step: 653 loss: 0.0186877757717\n",
      "step: 654 loss: 0.0162180927676\n",
      "step: 655 loss: 0.01699639117\n",
      "step: 656 loss: 0.017324710862\n",
      "step: 657 loss: 0.0171208380845\n",
      "step: 658 loss: 0.0165507338767\n",
      "step: 659 loss: 0.0158139065634\n",
      "step: 660 loss: 0.0175976396377\n",
      "step: 661 loss: 0.0157221627265\n",
      "step: 662 loss: 0.0153453070686\n",
      "step: 663 loss: 0.0168414958251\n",
      "step: 664 loss: 0.0158150333079\n",
      "step: 665 loss: 0.0173318825038\n",
      "step: 666 loss: 0.0174244368468\n",
      "step: 667 loss: 0.0172705476298\n",
      "step: 668 loss: 0.0168365563931\n",
      "step: 669 loss: 0.0173350037816\n",
      "step: 670 loss: 0.0174384622797\n",
      "step: 671 loss: 0.0174434379343\n",
      "step: 672 loss: 0.0151120846393\n",
      "step: 673 loss: 0.0168624339647\n",
      "step: 674 loss: 0.0163331767834\n",
      "step: 675 loss: 0.0168708900551\n",
      "step: 676 loss: 0.0170015671414\n",
      "step: 677 loss: 0.0170160400526\n",
      "step: 678 loss: 0.0180485337459\n",
      "step: 679 loss: 0.016806050245\n",
      "step: 680 loss: 0.0179225927772\n",
      "step: 681 loss: 0.0173352433633\n",
      "step: 682 loss: 0.0162395328742\n",
      "step: 683 loss: 0.0175276373781\n",
      "step: 684 loss: 0.0174619263584\n",
      "step: 685 loss: 0.0184873934644\n",
      "step: 686 loss: 0.016918397104\n",
      "step: 687 loss: 0.016391944439\n",
      "step: 688 loss: 0.0162443221056\n",
      "step: 689 loss: 0.0160512265365\n",
      "step: 690 loss: 0.016842575221\n",
      "step: 691 loss: 0.0163795357582\n",
      "step: 692 loss: 0.0170205417958\n",
      "step: 693 loss: 0.0154165234584\n",
      "step: 694 loss: 0.0160177632859\n",
      "step: 695 loss: 0.0189025508657\n",
      "step: 696 loss: 0.0177997255761\n",
      "step: 697 loss: 0.0164188801068\n",
      "step: 698 loss: 0.0175044835973\n",
      "step: 699 loss: 0.0168760220588\n",
      "step: 700 loss: 0.0180268859811\n",
      "step: 701 loss: 0.0169058297174\n",
      "step: 702 loss: 0.0181405924294\n",
      "step: 703 loss: 0.0163522810268\n",
      "step: 704 loss: 0.0172663772685\n",
      "step: 705 loss: 0.017009122213\n",
      "step: 706 loss: 0.0173714153268\n",
      "step: 707 loss: 0.0164778524043\n",
      "step: 708 loss: 0.0167111929498\n",
      "step: 709 loss: 0.0163070496107\n",
      "step: 710 loss: 0.0187254870252\n",
      "step: 711 loss: 0.0175326775212\n",
      "step: 712 loss: 0.0168313929928\n",
      "step: 713 loss: 0.0165116729131\n",
      "step: 714 loss: 0.017793663993\n",
      "step: 715 loss: 0.0171398055875\n",
      "step: 716 loss: 0.0180107695011\n",
      "step: 717 loss: 0.016975890358\n",
      "step: 718 loss: 0.0165917186242\n",
      "step: 719 loss: 0.017662169394\n",
      "step: 720 loss: 0.0165429097943\n",
      "step: 721 loss: 0.0170536601701\n",
      "step: 722 loss: 0.0162458903341\n",
      "step: 723 loss: 0.0173127512559\n",
      "step: 724 loss: 0.0175112813654\n",
      "step: 725 loss: 0.017846404547\n",
      "step: 726 loss: 0.0172917361483\n",
      "step: 727 loss: 0.0173910158051\n",
      "step: 728 loss: 0.0176215442271\n",
      "step: 729 loss: 0.0178459791506\n",
      "step: 730 loss: 0.0177318428627\n",
      "step: 731 loss: 0.018347867478\n",
      "step: 732 loss: 0.017652285618\n",
      "step: 733 loss: 0.0185220696253\n",
      "step: 734 loss: 0.0164976185172\n",
      "step: 735 loss: 0.0172892640329\n",
      "step: 736 loss: 0.0165945916324\n",
      "step: 737 loss: 0.0165289937808\n",
      "step: 738 loss: 0.0159893833436\n",
      "step: 739 loss: 0.0170392376953\n",
      "step: 740 loss: 0.0170458986749\n",
      "step: 741 loss: 0.016440231351\n",
      "step: 742 loss: 0.0175698305033\n",
      "step: 743 loss: 0.0163882565111\n",
      "step: 744 loss: 0.0171140439193\n",
      "step: 745 loss: 0.016935803681\n",
      "step: 746 loss: 0.0169351905798\n",
      "step: 747 loss: 0.0174491964598\n",
      "step: 748 loss: 0.0167891889757\n",
      "step: 749 loss: 0.0163303176814\n",
      "step: 750 loss: 0.0186923002061\n",
      "step: 751 loss: 0.0185515597191\n",
      "step: 752 loss: 0.0169546140402\n",
      "step: 753 loss: 0.0170095955805\n",
      "step: 754 loss: 0.017045635564\n",
      "step: 755 loss: 0.0178845676624\n",
      "step: 756 loss: 0.0168183002234\n",
      "step: 757 loss: 0.0170576407477\n",
      "step: 758 loss: 0.0174266887551\n",
      "step: 759 loss: 0.0163653045484\n",
      "step: 760 loss: 0.0166747634283\n",
      "step: 761 loss: 0.0163007734331\n",
      "step: 762 loss: 0.0158906181295\n",
      "step: 763 loss: 0.0165398527042\n",
      "step: 764 loss: 0.0170684242503\n",
      "step: 765 loss: 0.0170613175968\n",
      "step: 766 loss: 0.0167753943764\n",
      "step: 767 loss: 0.0169094628825\n",
      "step: 768 loss: 0.0182523727948\n",
      "step: 769 loss: 0.0168642955722\n",
      "step: 770 loss: 0.0160958090473\n",
      "step: 771 loss: 0.0164094083726\n",
      "step: 772 loss: 0.0167320079155\n",
      "step: 773 loss: 0.0159181809246\n",
      "step: 774 loss: 0.0168886496283\n",
      "step: 775 loss: 0.0170481185525\n",
      "step: 776 loss: 0.0166552225129\n",
      "step: 777 loss: 0.0176203311789\n",
      "step: 778 loss: 0.0169312954583\n",
      "step: 779 loss: 0.0161024305395\n",
      "step: 780 loss: 0.017107810814\n",
      "step: 781 loss: 0.0178318218284\n",
      "step: 782 loss: 0.0159479383806\n",
      "step: 783 loss: 0.0177762649963\n",
      "step: 784 loss: 0.0176637448168\n",
      "step: 785 loss: 0.0176970899788\n",
      "step: 786 loss: 0.017007380827\n",
      "step: 787 loss: 0.0172667272755\n",
      "step: 788 loss: 0.0177955987942\n",
      "step: 789 loss: 0.0183693345288\n",
      "step: 790 loss: 0.0166046568508\n",
      "step: 791 loss: 0.0166076822799\n",
      "step: 792 loss: 0.0179862198174\n",
      "step: 793 loss: 0.0167082527045\n",
      "step: 794 loss: 0.0174180396711\n",
      "step: 795 loss: 0.0173105837043\n",
      "step: 796 loss: 0.0168749983484\n",
      "step: 797 loss: 0.0173121220764\n",
      "step: 798 loss: 0.0174914395175\n",
      "step: 799 loss: 0.0169495151723\n",
      "step: 800 loss: 0.0169803541345\n",
      "step: 801 loss: 0.016739643199\n",
      "step: 802 loss: 0.0168595697209\n",
      "step: 803 loss: 0.0161895687904\n",
      "step: 804 loss: 0.0171622265719\n",
      "step: 805 loss: 0.0179073721705\n",
      "step: 806 loss: 0.0172280672065\n",
      "step: 807 loss: 0.0177425649291\n",
      "step: 808 loss: 0.0175435555259\n",
      "step: 809 loss: 0.0171116025259\n",
      "step: 810 loss: 0.0178660603253\n",
      "step: 811 loss: 0.0169056115188\n",
      "step: 812 loss: 0.0170358323376\n",
      "step: 813 loss: 0.0174317150576\n",
      "step: 814 loss: 0.0165901081677\n",
      "step: 815 loss: 0.0184332793935\n",
      "step: 816 loss: 0.0163036093098\n",
      "step: 817 loss: 0.0174343371532\n",
      "step: 818 loss: 0.0168861919906\n",
      "step: 819 loss: 0.0171137683705\n",
      "step: 820 loss: 0.0177440122485\n",
      "step: 821 loss: 0.0168899325504\n",
      "step: 822 loss: 0.0173051146266\n",
      "step: 823 loss: 0.0170780456393\n",
      "step: 824 loss: 0.0170831875286\n",
      "step: 825 loss: 0.0164827395693\n",
      "step: 826 loss: 0.0171213712749\n",
      "step: 827 loss: 0.0177668423189\n",
      "step: 828 loss: 0.0169060554965\n",
      "step: 829 loss: 0.0169203335224\n",
      "step: 830 loss: 0.0172118176675\n",
      "step: 831 loss: 0.0172265457585\n",
      "step: 832 loss: 0.0155671720026\n",
      "step: 833 loss: 0.0172967294319\n",
      "step: 834 loss: 0.0171502642324\n",
      "step: 835 loss: 0.0158024012747\n",
      "step: 836 loss: 0.0174163282948\n",
      "step: 837 loss: 0.017268638975\n",
      "step: 838 loss: 0.0174102960868\n",
      "step: 839 loss: 0.0184683208645\n",
      "step: 840 loss: 0.0172960596953\n",
      "step: 841 loss: 0.0174970640687\n",
      "step: 842 loss: 0.0179206733773\n",
      "step: 843 loss: 0.017004540145\n",
      "step: 844 loss: 0.0175233376318\n",
      "step: 845 loss: 0.0171219784393\n",
      "step: 846 loss: 0.019499014761\n",
      "step: 847 loss: 0.0161129175817\n",
      "step: 848 loss: 0.0182954094453\n",
      "step: 849 loss: 0.0165617067481\n",
      "step: 850 loss: 0.0166855284447\n",
      "step: 851 loss: 0.0171619077702\n",
      "step: 852 loss: 0.0168892166698\n",
      "step: 853 loss: 0.0174273371844\n",
      "step: 854 loss: 0.0177883245704\n",
      "step: 855 loss: 0.0169910223141\n",
      "step: 856 loss: 0.0176860179364\n",
      "step: 857 loss: 0.0176805867237\n",
      "step: 858 loss: 0.0177600689948\n",
      "step: 859 loss: 0.0170212885126\n",
      "step: 860 loss: 0.0176492339259\n",
      "step: 861 loss: 0.017243218609\n",
      "step: 862 loss: 0.0178399134528\n",
      "step: 863 loss: 0.0171725327587\n",
      "step: 864 loss: 0.0168373139786\n",
      "step: 865 loss: 0.0178073875068\n",
      "step: 866 loss: 0.016099622305\n",
      "step: 867 loss: 0.0183678604308\n",
      "step: 868 loss: 0.0156674849245\n",
      "step: 869 loss: 0.0184722999083\n",
      "step: 870 loss: 0.0173337938546\n",
      "step: 871 loss: 0.0167733043685\n",
      "step: 872 loss: 0.0157613876038\n",
      "step: 873 loss: 0.0168220933506\n",
      "step: 874 loss: 0.0171595753284\n",
      "step: 875 loss: 0.0171455673046\n",
      "step: 876 loss: 0.0165437525154\n",
      "step: 877 loss: 0.0182991907153\n",
      "step: 878 loss: 0.0168572368879\n",
      "step: 879 loss: 0.017381438811\n",
      "step: 880 loss: 0.0170163248892\n",
      "step: 881 loss: 0.0174268164631\n",
      "step: 882 loss: 0.0158191425271\n",
      "step: 883 loss: 0.01597720496\n",
      "step: 884 loss: 0.0162575202327\n",
      "step: 885 loss: 0.01741828185\n",
      "step: 886 loss: 0.0168901493634\n",
      "step: 887 loss: 0.0170768890206\n",
      "step: 888 loss: 0.0161729000188\n",
      "step: 889 loss: 0.0168810042873\n",
      "step: 890 loss: 0.0176701276301\n",
      "step: 891 loss: 0.0180900199525\n",
      "step: 892 loss: 0.0174728990183\n",
      "step: 893 loss: 0.0165185846965\n",
      "step: 894 loss: 0.0167621093048\n",
      "step: 895 loss: 0.0172433365563\n",
      "step: 896 loss: 0.016362112911\n",
      "step: 897 loss: 0.0162678200793\n",
      "step: 898 loss: 0.0178879461415\n",
      "step: 899 loss: 0.017768195128\n",
      "step: 900 loss: 0.0170722584501\n",
      "step: 901 loss: 0.017616483903\n",
      "step: 902 loss: 0.017252444076\n",
      "step: 903 loss: 0.0176145686269\n",
      "step: 904 loss: 0.017487716951\n",
      "step: 905 loss: 0.0171816629015\n",
      "step: 906 loss: 0.0174581304585\n",
      "step: 907 loss: 0.0170284260098\n",
      "step: 908 loss: 0.0166101712892\n",
      "step: 909 loss: 0.017095418173\n",
      "step: 910 loss: 0.0184944583598\n",
      "step: 911 loss: 0.0171346068911\n",
      "step: 912 loss: 0.015412512467\n",
      "step: 913 loss: 0.0177173418223\n",
      "step: 914 loss: 0.0165271487019\n",
      "step: 915 loss: 0.0179819331895\n",
      "step: 916 loss: 0.0168496782553\n",
      "step: 917 loss: 0.0163937003121\n",
      "step: 918 loss: 0.0169996365645\n",
      "step: 919 loss: 0.0174066335808\n",
      "step: 920 loss: 0.0161101594231\n",
      "step: 921 loss: 0.0156234638104\n",
      "step: 922 loss: 0.017979778684\n",
      "step: 923 loss: 0.0174410726905\n",
      "step: 924 loss: 0.0171717919857\n",
      "step: 925 loss: 0.0161003976039\n",
      "step: 926 loss: 0.0167931759664\n",
      "step: 927 loss: 0.0167002976376\n",
      "step: 928 loss: 0.0171997502671\n",
      "step: 929 loss: 0.0180315483755\n",
      "step: 930 loss: 0.0181503797585\n",
      "step: 931 loss: 0.0165686188662\n",
      "step: 932 loss: 0.0163281620795\n",
      "step: 933 loss: 0.0170091938386\n",
      "step: 934 loss: 0.0163567626155\n",
      "step: 935 loss: 0.0177810826156\n",
      "step: 936 loss: 0.0165232548156\n",
      "step: 937 loss: 0.0169325223709\n",
      "step: 938 loss: 0.0172993377564\n",
      "step: 939 loss: 0.0169216103944\n",
      "step: 940 loss: 0.0177265960868\n",
      "step: 941 loss: 0.0171590640629\n",
      "step: 942 loss: 0.0184774445174\n",
      "step: 943 loss: 0.0184351719498\n",
      "step: 944 loss: 0.0165213424824\n",
      "step: 945 loss: 0.0167977975442\n",
      "step: 946 loss: 0.018091924949\n",
      "step: 947 loss: 0.0166626358216\n",
      "step: 948 loss: 0.0179138387304\n",
      "step: 949 loss: 0.0174738920406\n",
      "step: 950 loss: 0.0166606671392\n",
      "step: 951 loss: 0.0166148192761\n",
      "step: 952 loss: 0.0171498806171\n",
      "step: 953 loss: 0.0166775415092\n",
      "step: 954 loss: 0.0152614527123\n",
      "step: 955 loss: 0.0171264158421\n",
      "step: 956 loss: 0.0166367516415\n",
      "step: 957 loss: 0.0164207828515\n",
      "step: 958 loss: 0.0170352176037\n",
      "step: 959 loss: 0.0180724638161\n",
      "step: 960 loss: 0.0184625312424\n",
      "step: 961 loss: 0.0171966581782\n",
      "step: 962 loss: 0.0180554197008\n",
      "step: 963 loss: 0.0156813550737\n",
      "step: 964 loss: 0.0174854193728\n",
      "step: 965 loss: 0.0177287657859\n",
      "step: 966 loss: 0.0167552478979\n",
      "step: 967 loss: 0.0160141037215\n",
      "step: 968 loss: 0.0170440322179\n",
      "step: 969 loss: 0.0175774279092\n",
      "step: 970 loss: 0.0168871182868\n",
      "step: 971 loss: 0.0177021020529\n",
      "step: 972 loss: 0.0175115029462\n",
      "step: 973 loss: 0.0158762570966\n",
      "step: 974 loss: 0.0183290315315\n",
      "step: 975 loss: 0.0177455539972\n",
      "step: 976 loss: 0.0166240553209\n",
      "step: 977 loss: 0.0172320852962\n",
      "step: 978 loss: 0.0176700790145\n",
      "step: 979 loss: 0.0176536824173\n",
      "step: 980 loss: 0.0176973711464\n",
      "step: 981 loss: 0.0170232661394\n",
      "step: 982 loss: 0.0174467094528\n",
      "step: 983 loss: 0.0175176126905\n",
      "step: 984 loss: 0.0185057386326\n",
      "step: 985 loss: 0.017864531188\n",
      "step: 986 loss: 0.0183681669937\n",
      "step: 987 loss: 0.0182462638138\n",
      "step: 988 loss: 0.0175362027073\n",
      "step: 989 loss: 0.0167547367328\n",
      "step: 990 loss: 0.0173987138562\n",
      "step: 991 loss: 0.0168759319024\n",
      "step: 992 loss: 0.0177377901145\n",
      "step: 993 loss: 0.0168320744741\n",
      "step: 994 loss: 0.0169419587212\n",
      "step: 995 loss: 0.0163193353407\n",
      "step: 996 loss: 0.017448512076\n",
      "step: 997 loss: 0.0163962192566\n",
      "step: 998 loss: 0.0176588204724\n",
      "step: 999 loss: 0.0175840993891\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh8I+RETU0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b973dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#R = get_data('ml-100k//u1.base')\n",
    "#test = get_data('ml-100k//u1.test')\n",
    "path = '/Users/limuyi/Desktop/git_4/ml-100k/'\n",
    "R = get_data('%su1.base'%path)\n",
    "test = get_data('%su1.test'%path)\n",
    "\n",
    "N,M = R.shape\n",
    "K = 5\n",
    "P = np.random.rand(N,K)\n",
    "Q = np.random.rand(M,K)\n",
    "step = 1000\n",
    "aP,aQ = matrix_factorization(R,P,Q,K,test,step)\n",
    "\n",
    "aR = np.dot(aP,aQ.T)\n",
    "\n",
    "# 画图\n",
    "plt.plot(step_list, loss_list,label='validation')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
